{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "### Algorithm Steps:\n",
    "1. Initialize Q-values (Q(s,a)) arbitrarily for all state-action pairs. \n",
    "2. For life or until learning is stopped:\n",
    "    3. choose action (a) in current world state (s) based on current Q-value estimates (Q(s,-)).\n",
    "    4. Take action (a) and observe outcome state (s') and reward (r)\n",
    "    5. Update Q(s,a) := Q(s,a) + alpha[r + gamma*maxalpha' Q(s',a') - Q(s,a)]\n",
    "    \n",
    "(3.perform action 5.measure reward 6.update Q)\n",
    "\n",
    "At the end of training, a \"good Q*table\" should have been produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Q-values\n",
    "* m columns (m = # of actions)\n",
    "* n rows (n = # of states)\n",
    "* Initialize values at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the total possible number of actions in our language learning script?\n",
    "\n",
    "We are navigating a vocabulary state space. In the other navigation examples, the possible actions are cardinal directions (East, West, North, and South). \n",
    "\n",
    "Our possible actions are ALL of the words suggested by Cozmo's Language Model. NO -- our possible actions are:\n",
    "1. Predict word.[exploitation]\n",
    "    * Take picture. \n",
    "    * use language model to make prediction. \n",
    "    * Record human feedback. \n",
    "2. Learn word. [exploration]\n",
    "    * take picture. \n",
    "    * record human label. \n",
    "3. [EMOTE.] \n",
    "4. [navigate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: (until learning is stopped; OR for life)\n",
    "steps 3 to 5 are repeated until a maximum # of episodes has been reached (specified by the user); or until we manually stop training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Choose an action\n",
    "Choose an action *a* in current state *s* based on current Q-value esimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploration / exploitation trade-off\n",
    "\n",
    "**epsilon greedy strategy**:\n",
    "1. Specify exploration rate epsilon = 1\n",
    "    * exploration rate 'epsilon' (rate of steps that we'll do randomly)\n",
    "        - set to 1 at beginning -- rate is highest at beginning b/c we don't know anything about values in Q-talbe -- higher rate favors exploration \n",
    "2. Generate random number\n",
    "    * If random number > epsilon, then do \"exploitation\"\n",
    "        - use what we already know to select best action at each step\n",
    "    * Else, do exploration\n",
    "    \n",
    "**Idea behind the strategy**: Start w/big epsilon at beginning of training of the Q-function; THEN, reduce it iteratively as the agent \"becomes more confident\" at estimating Q-values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4-5: Evaluate\n",
    "1. Take action *a* and observe outcome state *s'* and reward *r*.\n",
    "2. Update function Q(s,a)\n",
    "    * use Bellman equation: NewQ(s,a) = Q(s,a) + alpha[R(s,a) + gamma*max(Q'(s',a') - Q(s,a)]\n",
    "    \n",
    "New Q value for that state & action = \n",
    "\n",
    "Current Q value + \n",
    "\n",
    "Learning Rate * \n",
    "\n",
    "( Reward for taking that action at that state + \n",
    "\n",
    "Discount rate * \n",
    "\n",
    "max( Maximum expected future reward *given the new s' and all possible actions at that new state, a'*) - \n",
    "\n",
    "Current Q value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Bellman Equatioon\n",
    "'''\n",
    "New Q value = \n",
    "    Current Q value + \n",
    "    lr * [Reward + discount_rate * (highest Q value between\n",
    "    possible actions from the new sstate s' ) - Current Q value ]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The action value function (Q-function -- \"quality\" function) takes two inputs\":\n",
    "1. state\n",
    "2. action\n",
    "\n",
    "It returns:\n",
    "* expected future reward (\"of that action, at that state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_function(state, action):\n",
    "    \n",
    "    # if an action at a state is impossible\n",
    "    expected_future_reward = 0\n",
    "    \n",
    "    return expected_future_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
